{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "#import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "from keras_facenet import FaceNet\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "from IPython.utils import io\n",
    "\n",
    "import cv2\n",
    "\n",
    "#print the number of GPUs available\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people:  99\n",
      "Number of 0 labels:  40\n",
      "Number of 1 labels:  43\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pathLFW = './LFW/lfw_funneled/'\n",
    "\n",
    "#take the first 100 folders\n",
    "folders = os.listdir(pathLFW)[:100]\n",
    "\n",
    "#the idea is to create a dataset for a siamese network. We have to create a list of tuples (image1, image2, label) where label is 1 if the two images are of the same person and 0 otherwise\n",
    "#if there is a person with only one image, we discard use it for the training in the false case\n",
    "\n",
    "#create a dictionary with the name of the person as key and the list of images as value\n",
    "diz = {}\n",
    "for folder in folders:\n",
    "    #check if its actually a folder\n",
    "    if not os.path.isdir(pathLFW + folder):\n",
    "        continue\n",
    "    #read the RGB images\n",
    "    images = [cv2.imread(pathLFW + folder + '/' + img) for img in os.listdir(pathLFW + folder) if img.endswith('.jpg')]\n",
    "    diz[folder] = images\n",
    "\n",
    "\n",
    "train_x = []\n",
    "train_y = []\n",
    "\n",
    "\n",
    "print('Number of people: ', len(diz))\n",
    "\n",
    "firstPair = False\n",
    "\n",
    "\n",
    "\n",
    "for nome in diz.keys():\n",
    "    if len(diz[nome]) == 1:\n",
    "        #print(\"ciao\")\n",
    "        if not firstPair:   #if it was already been found a person with only one image then we have created a tuple\n",
    "            img1 = diz[nome][0]\n",
    "            #resize the image into a 160x160 image\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "            firstPair = True\n",
    "            \n",
    "        else:\n",
    "            img2 = diz[nome][0]\n",
    "            #resize the image into a 160x160 image\n",
    "            \n",
    "            tupla = (img1, img2)\n",
    "            train_x.append(tupla)\n",
    "            train_y.append(0)\n",
    "            firstPair = False\n",
    "    else:\n",
    "        #print(\"ciao2\")\n",
    "        #create a for with step 2\n",
    "        for i in range(0, len(diz[nome]), 2):\n",
    "            #check if i + 1 is out of range\n",
    "            if i + 1 >= len(diz[nome]):\n",
    "                if not firstPair:   #if it was already been found a person with only one image then we have created a tuple\n",
    "                    img1 = diz[nome][i]\n",
    "                    \n",
    "                    firstPair = True\n",
    "                    \n",
    "                else:\n",
    "                    img2 = diz[nome][i]\n",
    "                    \n",
    "                    tupla = (img1, img2)\n",
    "                    train_x.append(tupla)\n",
    "                    train_y.append(0)\n",
    "                    firstPair = False\n",
    "                continue\n",
    "            img1 = diz[nome][i]\n",
    "           \n",
    "\n",
    "            img2 = diz[nome][i+1]\n",
    "        \n",
    "            tupla = (img1, img2)\n",
    "            train_x.append(tupla)\n",
    "            train_y.append(1)\n",
    "\n",
    "\n",
    "print('Number of 0 labels: ', train_y.count(0))\n",
    "print('Number of 1 labels: ', train_y.count(1))\n",
    "\n",
    "#train x in a numpy array\n",
    "train_x = np.array(train_x)\n",
    "train_y = np.array(train_y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0\n",
      "2023-01-27 13:01:37.941467: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n",
      "2023-01-27 13:01:38.956810: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-01-27 13:01:38.957903: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:234] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.6\n",
      "2023-01-27 13:01:38.957914: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:237] Used ptxas at ptxas\n",
      "2023-01-27 13:01:38.957958: W tensorflow/compiler/xla/stream_executor/gpu/redzone_allocator.cc:318] UNIMPLEMENTED: ptxas ptxas too old. Falling back to the driver to compile.\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2023-01-27 13:01:40.530149: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1191530/465954047.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_embeddings_x = np.array(train_embeddings_x)\n"
     ]
    }
   ],
   "source": [
    "embedder = FaceNet()\n",
    "\n",
    "#set verbose to 0 to avoid printing the progress bar in tensorflow\n",
    "\n",
    "with io.capture_output(stderr=False) as captured:\n",
    "    train_embeddings_x = []\n",
    "    for i in range(train_x.shape[0]):\n",
    "        # Print to stderr\n",
    "        print(i, file=sys.stderr)\n",
    "        \n",
    "        print(train_x[i][0].shape)\n",
    "        print(train_x[i][1].shape)\n",
    "        emb1 = embedder.extract(train_x[i][0], threshold=0.95)\n",
    "        emb2 = embedder.extract(train_x[i][1], threshold=0.95)\n",
    "        train_embeddings_x.append((emb1, emb2))\n",
    "\n",
    "    train_embeddings_x = np.array(train_embeddings_x)\n",
    "\n",
    "print(train_embeddings_x.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save train embeddings and train y\n",
    "np.save('./embeddings/train_embeddings_x.npy', train_embeddings_x)\n",
    "np.save('./embeddings/train_y.npy', train_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('si-cctv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c73208cc7c35a2399647dfd68c969190c7caf105886c1e4d458f6f5ad06c724e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
